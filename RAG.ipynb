{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMDqd3RAVmGrJEMjYzEW2EE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SegaObin/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVpSQdVVr-JL"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "a_s2K6LGJ75P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HhDRCW7bt3Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных, данные через гптшку циклом нагенерил"
      ],
      "metadata": {
        "id": "BC5sGG92GM12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/RAG/incidents.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "print(len(data))\n",
        "data[0]"
      ],
      "metadata": {
        "id": "0YL8WCd1twe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединю самари и описание проблемы, чтобы их было удобнее искать в векторной БД"
      ],
      "metadata": {
        "id": "MyB6ySbLGXuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_strings = [f'{item['summary']}. {item['description']}' for item in data]\n",
        "search_strings[0]"
      ],
      "metadata": {
        "id": "MXf4gHnvu4FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_strings[3]"
      ],
      "metadata": {
        "id": "7jxY7u_dwan-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаю из этого эмбединги"
      ],
      "metadata": {
        "id": "LfbOXisCGuh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
        "embeddings = model.encode(search_strings, show_progress_bar=True)"
      ],
      "metadata": {
        "id": "nR3qdpGtwgac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорчу векторную БД FAISS, и закидываю туда эмбединги всех проблем (саммари и описание проблемы)"
      ],
      "metadata": {
        "id": "wpCmWO2PG0Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings.astype('float32'))"
      ],
      "metadata": {
        "id": "6zwy4csvy72z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.d"
      ],
      "metadata": {
        "id": "Pvo56MrY1iK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код для поиска по БД и считываение полной инфы о топ k проблемах из прошлого"
      ],
      "metadata": {
        "id": "YY97HgIhISn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_incedents(query_text, top_k=3):\n",
        "  query_emb = model.encode([query_text]).astype('float32')\n",
        "  distances, indices = index.search(query_emb, top_k)\n",
        "\n",
        "  result = []\n",
        "  for idx in indices[0]:\n",
        "    incident = data[idx]\n",
        "    result.append({\n",
        "        'incident_id': incident['incident_id'],\n",
        "        'summary': incident['summary'],\n",
        "        'description': incident['description'],\n",
        "        'resolution': incident['resolution']\n",
        "    })\n",
        "\n",
        "  return result\n",
        "\n",
        "user_query = 'Закончилась память на диске'\n",
        "similar_cases = find_similar_incedents(user_query)"
      ],
      "metadata": {
        "id": "hywTIvFh2Koi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_cases"
      ],
      "metadata": {
        "id": "pzCImr3z2ZRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"Qwen/Qwen3-4B\"\n",
        "\n",
        "# load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "XtkSKzHn2egx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание пропта для корректной работы RAG и понимания моедли че делать + настройка модели (температура, пенальти и тд) + обрезка предложения до точки на всякий случай потому что иногда модель генерит до талого"
      ],
      "metadata": {
        "id": "7urBj2KCI1zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_summary(query, similar_incidents):\n",
        "    context = \"\"\n",
        "    for i, inc in enumerate(similar_incidents[:3]):\n",
        "        context += f\"- {inc['resolution']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "Ты — лучший работник техподдержки. Твоя задача — дать максимально лаконичный ответ.\n",
        "ЗАПРЕЩЕНО: использовать <think>, рассуждать, писать вступления.\n",
        "Формат:\n",
        "  1. НАПИШИ МИНИМУМ ТРИ ВАРИАНТА решения проблемы на основе опыта прошлых лет и твоих знаний.\n",
        "  2. ОБЯЗАТЕЛЬНО пиши ответ в формате \"Варианты решения вашей проблемы: 1).. 2).. 3)..\"\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Проблема с которой столкнулся пользователь: {query}\n",
        "Опыт прошлых лет: {context}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Варианты решения Вашей проблемы: \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = llm_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.3,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Извлекаем ответ\n",
        "    answer = \"Варианты решения Вашей проблемы: \" + decoded_output.split(\"Варианты решения Вашей проблемы:\")[-1].strip()\n",
        "\n",
        "    # Очистка от мусора\n",
        "    answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL)\n",
        "    answer = answer.replace('</think>', '').strip()\n",
        "\n",
        "    # ФИКС ЛОГИЧЕСКОГО ЗАВЕРШЕНИЯ:\n",
        "    # Оставляем только текст до последней точки, чтобы убрать возможный обрыв\n",
        "    if \".\" in answer:\n",
        "        answer = answer[:answer.rfind(\".\")+1]\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "upfPsxZX9rOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общая сборка всего, сначала находим похожие кейсы, генерим самари при помощи LLM, если хотм посмотреть ответ в ручном режиме то указываем silent=False это нужно чтобы нормлаьно реализовать проверку LLM-кой"
      ],
      "metadata": {
        "id": "w5kTebNrJMJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_new_incident(user_query, num_k, silent=True):\n",
        "    similar_cases = find_similar_incedents(user_query, top_k=num_k)\n",
        "\n",
        "    llm_advice = generate_summary(user_query, similar_cases)\n",
        "\n",
        "    if not silent:\n",
        "        print(f\"=== ОТЧЕТ ДЛЯ K={num_k} ===\")\n",
        "        import pandas as pd\n",
        "        display(pd.DataFrame(similar_cases))\n",
        "        print(\"\\nРЕКОМЕНДАЦИЯ:\")\n",
        "        print(llm_advice)\n",
        "        print(\"-\" * 30)\n",
        "    else:\n",
        "      return llm_advice\n",
        "\n",
        "process_new_incident(\"Пользователи жалуются на медленную работу базы данных\", num_k=5, silent=False)"
      ],
      "metadata": {
        "id": "fp9QiwjVeqvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверяю результат при помощи этой же LLM, но с другим промтов, для нескольких промтов пользователей по разным темам"
      ],
      "metadata": {
        "id": "WtbAHnvdJ-RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompts = [\n",
        "    'Очень быстро растет потребление памяти.',\n",
        "    'Закончилось место на диске.',\n",
        "    'База данных долго отвечает на запросы.',\n",
        "]"
      ],
      "metadata": {
        "id": "5EURu5A0hrlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for query in user_prompts:\n",
        "\n",
        "  model_ans_context_2 = process_new_incident(query, num_k=2)\n",
        "  model_ans_context_5 = process_new_incident(query, num_k=5)\n",
        "  model_ans_context_10 = process_new_incident(query, num_k=10)\n",
        "\n",
        "  eval_prompt = f\"\"\"<|im_start|>system\n",
        "Ты — аудитор технической поддержки. Сравни три ответа работника на запрос клиента в зависимости от того сколько похожих событий из прошлого сотрудник знает.\n",
        "Запрос пользователя: {query}\n",
        "Ответ 1 (сотрудник рассмотрел 2 инцидента): {model_ans_context_2}\n",
        "Ответ 2 (сотрудник рассмотрел 5 инцидентов): {model_ans_context_5}\n",
        "Ответ 3 (сотрудник рассмотрел 10 инцидентов): {model_ans_context_10}\n",
        "Какой ответ более полный и точный? Оцени каждый ответ по шкале от одного до 5. Потом напиши итог какой ответ ты выбираешь как самый лучший.<|im_end|>\n",
        "<|im_start|>assistant\n",
        "Вот оценка каждого ответа сотрудника:\n",
        "\"\"\"\n",
        "\n",
        "  inputs_eval = tokenizer(eval_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  outputs_eval = llm_model.generate(\n",
        "      **inputs_eval,\n",
        "      max_new_tokens=250,      # Уменьшаем лимит, чтобы не было места для дублей\n",
        "      temperature=0.4,        # Минимальная температура для исключения повторов\n",
        "      repetition_penalty=1.3, # Повышаем штраф за повторение одних и тех же слов\n",
        "      eos_token_id=tokenizer.eos_token_id,\n",
        "      pad_token_id=tokenizer.eos_token_id\n",
        "  )\n",
        "\n",
        "  decoded_output_eval = tokenizer.decode(outputs_eval[0], skip_special_tokens=True)\n",
        "\n",
        "  answer_eval = \"Вот оценка каждого ответа сотрудника: \" + decoded_output_eval.split(\"Вот оценка каждого ответа сотрудника:\")[-1].strip()\n",
        "\n",
        "  if \".\" in answer_eval:\n",
        "      answer_eval = answer_eval[:answer_eval.rfind(\".\")+1]\n",
        "\n",
        "  print(10 * '=' + f'ОЦЕНКА РЕЗУЛЬТАТА ДЛЯ ПРОМПТА \"{query}\"' + 50 * '=')\n",
        "  print(answer_eval)\n",
        "  print(60 * '=')\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "id": "aEpE7BwjsKnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrL3SGcMr8xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nv9Ger6vVYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}